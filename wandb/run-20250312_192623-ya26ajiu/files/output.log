Fold 1 | Epoch [1/80] | Train Loss: 0.0248 | Val Loss: 0.0175 | R2: -0.0194 | Pearson: 0.0391
Fold 1 | Epoch [2/80] | Train Loss: 0.0194 | Val Loss: 0.0181 | R2: -0.0607 | Pearson: 0.0549
Fold 1 | Epoch [3/80] | Train Loss: 0.0191 | Val Loss: 0.0193 | R2: -0.1264 | Pearson: 0.1646
Fold 1 | Epoch [4/80] | Train Loss: 0.0180 | Val Loss: 0.0168 | R2: 0.0153 | Pearson: 0.2004
Fold 1 | Epoch [5/80] | Train Loss: 0.0175 | Val Loss: 0.0388 | R2: -1.2690 | Pearson: 0.1209
Fold 1 | Epoch [6/80] | Train Loss: 0.0167 | Val Loss: 0.0156 | R2: 0.0900 | Pearson: 0.3695
Fold 1 | Epoch [7/80] | Train Loss: 0.0152 | Val Loss: 0.0169 | R2: 0.0198 | Pearson: 0.3794
Fold 1 | Epoch [8/80] | Train Loss: 0.0141 | Val Loss: 0.0216 | R2: -0.2693 | Pearson: 0.2394
Fold 1 | Epoch [9/80] | Train Loss: 0.0134 | Val Loss: 0.0113 | R2: 0.3340 | Pearson: 0.5945
Fold 1 | Epoch [10/80] | Train Loss: 0.0120 | Val Loss: 0.0218 | R2: -0.2698 | Pearson: 0.4163
Fold 1 | Epoch [11/80] | Train Loss: 0.0109 | Val Loss: 0.0256 | R2: -0.5051 | Pearson: 0.2055
Fold 1 | Epoch [12/80] | Train Loss: 0.0097 | Val Loss: 0.0227 | R2: -0.3289 | Pearson: 0.4770
Fold 1 | Epoch [13/80] | Train Loss: 0.0089 | Val Loss: 0.0485 | R2: -1.8415 | Pearson: 0.2914
Fold 1 | Epoch [14/80] | Train Loss: 0.0073 | Val Loss: 0.0114 | R2: 0.3302 | Pearson: 0.6077
Fold 1 | Epoch [15/80] | Train Loss: 0.0059 | Val Loss: 0.0127 | R2: 0.2536 | Pearson: 0.5040
Fold 1 | Epoch [16/80] | Train Loss: 0.0048 | Val Loss: 0.0111 | R2: 0.3470 | Pearson: 0.6340
Fold 1 | Epoch [17/80] | Train Loss: 0.0040 | Val Loss: 0.0161 | R2: 0.0575 | Pearson: 0.6199
Fold 1 | Epoch [18/80] | Train Loss: 0.0038 | Val Loss: 0.0120 | R2: 0.3021 | Pearson: 0.6249
Fold 1 | Epoch [19/80] | Train Loss: 0.0030 | Val Loss: 0.0162 | R2: 0.0531 | Pearson: 0.5711
Fold 1 | Epoch [20/80] | Train Loss: 0.0029 | Val Loss: 0.0100 | R2: 0.4143 | Pearson: 0.6524
Fold 1 | Epoch [21/80] | Train Loss: 0.0024 | Val Loss: 0.0114 | R2: 0.3329 | Pearson: 0.6032
Fold 1 | Epoch [22/80] | Train Loss: 0.0019 | Val Loss: 0.0097 | R2: 0.4311 | Pearson: 0.6732
Fold 1 | Epoch [23/80] | Train Loss: 0.0018 | Val Loss: 0.0089 | R2: 0.4746 | Pearson: 0.7105
Fold 1 | Epoch [24/80] | Train Loss: 0.0018 | Val Loss: 0.0127 | R2: 0.2652 | Pearson: 0.5284
Fold 1 | Epoch [25/80] | Train Loss: 0.0016 | Val Loss: 0.0084 | R2: 0.5100 | Pearson: 0.7159
Fold 1 | Epoch [26/80] | Train Loss: 0.0015 | Val Loss: 0.0100 | R2: 0.4167 | Pearson: 0.6532
Fold 1 | Epoch [27/80] | Train Loss: 0.0014 | Val Loss: 0.0136 | R2: 0.2016 | Pearson: 0.4732
Fold 1 | Epoch [28/80] | Train Loss: 0.0012 | Val Loss: 0.0088 | R2: 0.4826 | Pearson: 0.7190
Fold 1 | Epoch [29/80] | Train Loss: 0.0013 | Val Loss: 0.0162 | R2: 0.0486 | Pearson: 0.2218
Fold 1 | Epoch [30/80] | Train Loss: 0.0011 | Val Loss: 0.0085 | R2: 0.5006 | Pearson: 0.7080
Fold 1 | Epoch [31/80] | Train Loss: 0.0010 | Val Loss: 0.0133 | R2: 0.2174 | Pearson: 0.6626
Fold 1 | Epoch [32/80] | Train Loss: 0.0009 | Val Loss: 0.0092 | R2: 0.4627 | Pearson: 0.6818
Fold 1 | Epoch [33/80] | Train Loss: 0.0008 | Val Loss: 0.0091 | R2: 0.4634 | Pearson: 0.6874
Fold 1 | Epoch [34/80] | Train Loss: 0.0011 | Val Loss: 0.0092 | R2: 0.4599 | Pearson: 0.6887
Fold 1 | Epoch [35/80] | Train Loss: 0.0009 | Val Loss: 0.0099 | R2: 0.4181 | Pearson: 0.6948
Fold 1 | Epoch [36/80] | Train Loss: 0.0007 | Val Loss: 0.0081 | R2: 0.5279 | Pearson: 0.7269
Fold 1 | Epoch [37/80] | Train Loss: 0.0005 | Val Loss: 0.0080 | R2: 0.5311 | Pearson: 0.7303
Fold 1 | Epoch [38/80] | Train Loss: 0.0005 | Val Loss: 0.0085 | R2: 0.5075 | Pearson: 0.7144
Fold 1 | Epoch [39/80] | Train Loss: 0.0005 | Val Loss: 0.0093 | R2: 0.4518 | Pearson: 0.6737
Fold 1 | Epoch [40/80] | Train Loss: 0.0005 | Val Loss: 0.0093 | R2: 0.4582 | Pearson: 0.7112
Fold 1 | Epoch [41/80] | Train Loss: 0.0005 | Val Loss: 0.0114 | R2: 0.3334 | Pearson: 0.6690
Fold 1 | Epoch [42/80] | Train Loss: 0.0005 | Val Loss: 0.0090 | R2: 0.4711 | Pearson: 0.6938
Fold 1 | Epoch [43/80] | Train Loss: 0.0004 | Val Loss: 0.0086 | R2: 0.4981 | Pearson: 0.7137
Fold 1 | Epoch [44/80] | Train Loss: 0.0004 | Val Loss: 0.0086 | R2: 0.4974 | Pearson: 0.7170
Fold 1 | Epoch [45/80] | Train Loss: 0.0003 | Val Loss: 0.0082 | R2: 0.5210 | Pearson: 0.7278
Fold 1 | Epoch [46/80] | Train Loss: 0.0003 | Val Loss: 0.0080 | R2: 0.5308 | Pearson: 0.7320
Fold 1 | Epoch [47/80] | Train Loss: 0.0003 | Val Loss: 0.0079 | R2: 0.5389 | Pearson: 0.7424
Fold 1 | Epoch [48/80] | Train Loss: 0.0003 | Val Loss: 0.0084 | R2: 0.5069 | Pearson: 0.7146
Fold 1 | Epoch [49/80] | Train Loss: 0.0002 | Val Loss: 0.0082 | R2: 0.5231 | Pearson: 0.7287
Fold 1 | Epoch [50/80] | Train Loss: 0.0002 | Val Loss: 0.0081 | R2: 0.5260 | Pearson: 0.7332
Fold 1 | Epoch [51/80] | Train Loss: 0.0002 | Val Loss: 0.0080 | R2: 0.5357 | Pearson: 0.7377
Fold 1 | Epoch [52/80] | Train Loss: 0.0002 | Val Loss: 0.0078 | R2: 0.5403 | Pearson: 0.7413
Fold 1 | Epoch [53/80] | Train Loss: 0.0002 | Val Loss: 0.0079 | R2: 0.5373 | Pearson: 0.7388
Fold 1 | Epoch [54/80] | Train Loss: 0.0002 | Val Loss: 0.0079 | R2: 0.5394 | Pearson: 0.7390
Fold 1 | Epoch [55/80] | Train Loss: 0.0002 | Val Loss: 0.0079 | R2: 0.5385 | Pearson: 0.7393
Fold 1 | Epoch [56/80] | Train Loss: 0.0001 | Val Loss: 0.0078 | R2: 0.5401 | Pearson: 0.7404
Fold 1 | Epoch [57/80] | Train Loss: 0.0001 | Val Loss: 0.0079 | R2: 0.5358 | Pearson: 0.7376
Fold 1 | Epoch [58/80] | Train Loss: 0.0001 | Val Loss: 0.0079 | R2: 0.5366 | Pearson: 0.7371
Fold 1 | Epoch [59/80] | Train Loss: 0.0001 | Val Loss: 0.0077 | R2: 0.5448 | Pearson: 0.7422
Fold 1 | Epoch [60/80] | Train Loss: 0.0001 | Val Loss: 0.0079 | R2: 0.5350 | Pearson: 0.7372
Process Process-1:
Traceback (most recent call last):
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\multiprocessing\process.py", line 315, in _bootstrap
    self.run()
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\multiprocessing\process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\hyunoh\Documents\Codes\BMD_code\train.py", line 47, in train_fold
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)
  File "C:\Users\hyunoh\Documents\Codes\BMD_code\train.py", line 71, in train_one_epoch
    for images, labels, _ in train_loader:
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\site-packages\torch\utils\data\dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "C:\Users\hyunoh\Documents\Codes\BMD_code\regression_function_classes.py", line 213, in __getitem__
    image = self.process_image(img_path)
  File "C:\Users\hyunoh\Documents\Codes\BMD_code\regression_function_classes.py", line 176, in process_image
    image = np.load(img_path)
  File "C:\Users\hyunoh\anaconda3\envs\spine\lib\site-packages\numpy\lib\npyio.py", line 405, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/dxa_data/gt_npy_box\\1540161_6_L3.npy'
